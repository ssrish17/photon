From 50968927a1645b068c8e3aecb3b286f768376e67 Mon Sep 17 00:00:00 2001
From: Alexey Makhalov <amakhalov@vmware.com>
Date: Mon, 7 Aug 2023 16:48:33 +0000
Subject: [PATCH] Enable and enhance SCHED isolation

CPU isolation code has several types of users or kernel subsystem such
as a sched domain, timer, workqueue, etc. See enum hk_flags in
include/linux/sched/isolation.h for full list of isolation types.

Separation on several types was introduced long time ago by upstream
commit de201559df872f83d0c08 ("sched/isolation: Introduce housekeeping
flags"). This commit introduced HK_FLAG_SCHED type, but there was no
path of kernel paramater to activate it. HK_FLAG_SCHED isolation remains
dead code even now. Some aspects of scheduler isolation to avoid processes
on isolcpus such as migration or load balancing disablement on isolcpus
are covered by HK_FLAG_DOMAIN. But there are still cases where we can see
processes scheduled on isolcpus. Example, initial placement of the
process when its cpuset is mix of isol and non isol cpus can pick isolcpu.
The process will remain there for entire life of until user force move it.
It won't be automatically migrated as load balancer does not touch
isolcpus.

This patch enables HK_FLAG_SCHED and enhances corresponding sched isolation
to do not put any processes with fair sched class to isolcpus. The main
goal of this patch is to improve initial placement of the processes within
a cpuset. There are 3 possible scenarios:
 1. No isolcpus within a target cpuset - pick first or any cpu, depending
    on enablement of distribution logic.
 2. Some CPUs within cpuset are isolated - pick a non isolated CPU.
 3. All CPUs within cpuset are isolated - pick first or any cpum depending
    on enablement of distribution logic.
Note, sched isolation only works for non realtime processes with fair
scheduling class.

To activate sched isolation "sched" flag must be passed to the isolcpus=
kernel parameter. Example: "isolcpus=domain,sched,managed_irq,2-3"

Signed-off-by: Alexey Makhalov <amakhalov@vmware.com>
---
 kernel/sched/core.c      | 11 +++++++++++
 kernel/sched/fair.c      | 19 ++++++++++++++++++-
 kernel/sched/isolation.c |  6 ++++++
 3 files changed, 35 insertions(+), 1 deletion(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 9d6dd14cf..7ed516b1a 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1886,6 +1886,8 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 {
 	const struct cpumask *cpu_allowed_mask = task_cpu_possible_mask(p);
 	const struct cpumask *cpu_valid_mask = cpu_active_mask;
+	struct cpumask cpu_valid_submask;
+	const struct cpumask *hk_mask = housekeeping_cpumask(HK_TYPE_SCHED);
 	bool kthread = p->flags & PF_KTHREAD;
 	struct cpumask *user_mask = NULL;
 	unsigned int dest_cpu;
@@ -1913,6 +1915,15 @@ static int __set_cpus_allowed_ptr_locked(struct task_struct *p,
 		}
 	}
 
+	/*
+	 * dest_cpu should not use isolcpus if housekeeping cpus are present in
+	 * new_mask. Reduce cpu_valid_mask to housekeeping subset.
+	 */
+	if (cpumask_intersects(new_mask, hk_mask)) {
+		cpumask_and(&cpu_valid_submask, cpu_valid_mask, hk_mask);
+		cpu_valid_mask = &cpu_valid_submask;
+	}
+
 	/*
 	 * Picking a ~random cpu helps in cases where we are changing affinity
 	 * for groups of tasks (ie. cpuset), so that load balancing is not
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 45c1d03af..5a30d51bb 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -5707,6 +5707,9 @@ enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags)
 	int idle_h_nr_running = task_has_idle_policy(p);
 	int task_new = !(flags & ENQUEUE_WAKEUP);
 
+	WARN(!housekeeping_test_cpu(cpu_of(rq), HK_TYPE_SCHED) &&
+		!(p->flags & PF_KTHREAD),
+		"Enqueue %s(%d) on cpu %d", p->comm, p->pid, cpu_of(rq));
 	/*
 	 * The code below (indirectly) updates schedutil which looks at
 	 * the cfs_rq utilization to select a frequency.
@@ -7084,6 +7087,24 @@ static void task_dead_fair(struct task_struct *p)
 {
 	remove_entity_load_avg(&p->se);
 }
+
+static void
+set_cpus_allowed_fair(struct task_struct *p, const struct cpumask *new_mask, u32 flags)
+{
+	if (housekeeping_enabled(HK_TYPE_SCHED) &&
+	    !(p->flags & PF_KTHREAD) &&
+	    !(flags & (SCA_MIGRATE_ENABLE | SCA_MIGRATE_DISABLE))) {
+		const struct cpumask *hk_mask = housekeeping_cpumask(HK_TYPE_SCHED);
+
+		if (cpumask_intersects(new_mask, hk_mask)) {
+			cpumask_and(&p->cpus_mask, new_mask, hk_mask);
+			p->nr_cpus_allowed = cpumask_weight(&p->cpus_mask);
+			return;
+		}
+	}
+
+	set_cpus_allowed_common(p, new_mask, flags);
+}
 
 static int
 balance_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
@@ -11546,7 +11563,7 @@ const struct sched_class fair_sched_class
 	.rq_offline		= rq_offline_fair,
 
 	.task_dead		= task_dead_fair,
-	.set_cpus_allowed	= set_cpus_allowed_common,
+	.set_cpus_allowed	= set_cpus_allowed_fair,
 #endif
 
 	.task_tick		= task_tick_fair,
diff --git a/kernel/sched/isolation.c b/kernel/sched/isolation.c
index 5a6ea03f9..d82107b15 100644
--- a/kernel/sched/isolation.c
+++ b/kernel/sched/isolation.c
@@ -167,6 +167,12 @@ static int __init housekeeping_isolcpus_setup(char *str)
 			continue;
 		}
 
+		if (!strncmp(str, "sched,", 6)) {
+			str += 6;
+			flags |= HK_FLAG_SCHED;
+			continue;
+		}
+
 		if (!strncmp(str, "domain,", 7)) {
 			str += 7;
 			flags |= HK_FLAG_DOMAIN;
-- 
2.30.3

